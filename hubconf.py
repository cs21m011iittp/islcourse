# -*- coding: utf-8 -*-
"""hubconf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yPcwLCzBeSn3PzwVhc_lOnDvak_h-kvr
"""

import torch
from torch import nn
import torch.optim as optim
from sklearn.datasets import load_digits

# You can import whatever standard packages are required

# full sklearn, full pytorch, pandas, matplotlib, numpy are all available
# Ideally you do not need to pip install any other packages!
# Avoid pip install requirement on the evaluation program side, if you use above packages and sub-packages of them, then that is fine!

###part1###
def get_data_blobs(n_points=100):
  X, y = make_blobs(n_samples=n_points, centers=3, n_features=2,random_state=0)
  return X,y

def get_data_circles(n_points=100):
  X, y = make_circles(n_samples = n_points)
  return X,y

def get_data_mnist():
  X,y = load_digits(return_X_y=True)
  #print(X.shape)
  return X,y

def build_kmeans(X=None,k=10):
  kmeans = KMeans(n_clusters=k, random_state=0).fit(X)
  return kmeans

def assign_kmeans(km=None,X=None):
  ypred = km.predict(X)
  return ypred

def compare_clusterings(ypred_1=None,ypred_2=None):
  h = homogeneity_score(ypred_1,ypred_2)
  c =completeness_score(ypred_1,ypred_2)
  v = v_measure_score(ypred_1,ypred_2)
  return h,c,v







###### PART 2 ######

def build_lr_model(X=None, y=None):
  pass
  lr_model = None
  # write your code...
  # Build logistic regression, refer to sklearn
  return lr_model

def build_rf_model(X=None, y=None):
  pass
  rf_model = None
  # write your code...
  # Build Random Forest classifier, refer to sklearn
  return rf_model

def get_metrics(model=None,X=None,y=None):
  pass
  # Obtain accuracy, precision, recall, f1score, auc score - refer to sklearn metrics
  acc, prec, rec, f1, auc = 0,0,0,0,0
  # write your code here...
  return acc, prec, rec, f1, auc

def get_paramgrid_lr():
  # you need to return parameter grid dictionary for use in grid search cv
  # penalty: l1 or l2
  lr_param_grid = None
  # refer to sklearn documentation on grid search and logistic regression
  # write your code here...
  return lr_param_grid

def get_paramgrid_rf():
  # you need to return parameter grid dictionary for use in grid search cv
  # n_estimators: 1, 10, 100
  # criterion: gini, entropy
  # maximum depth: 1, 10, None  
  rf_param_grid = None
  # refer to sklearn documentation on grid search and random forest classifier
  # write your code here...
  return rf_param_grid

def perform_gridsearch_cv_multimetric(model=None, param_grid=None, cv=5, X=None, y=None, metrics=['accuracy','roc_auc']):
  
  # you need to invoke sklearn grid search cv function
  # refer to sklearn documentation
  # the cv parameter can change, ie number of folds  
  
  # metrics = [] the evaluation program can change what metrics to choose
  
  grid_search_cv = None
  # create a grid search cv object
  # fit the object on X and y input above
  # write your code here...
  
  # metric of choice will be asked here, refer to the-scoring-parameter-defining-model-evaluation-rules of sklearn documentation
  
  # refer to cv_results_ dictonary
  # return top 1 score for each of the metrics given, in the order given in metrics=... list
  
  top1_scores = []
  
  return top1_scores

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor, ToPILImage
from PIL import Image
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')

"""# Part3"""

def load_data():
    train_data = datasets.MNIST(
        root="data",
        train=True,
        download=True,
        transform=ToTensor(), 
    )

  # Download test data from open datasets.
    test_data = datasets.MNIST(
        root="data",
        train=False,
        download=True,
        transform=ToTensor(),
    )

    return train_data, test_data

train_data,test_data=load_data()

print('size of X vector: ',train_data[0][0].shape)
print('size of Y vector:',len(set([y for x,y in train_data])))

def create_dataloaders(train_data, test_data, batch_size=32):
    # Create data loaders.
    train_dataloader = DataLoader(train_data, batch_size=batch_size)
    test_dataloader = DataLoader(test_data, batch_size=batch_size)

    for X, y in train_dataloader:
        print(f"Shape of X [N, C, H, W]: {X.shape}")
        print(f"Shape of y: {y.shape} {y.dtype}")
        break
        
    return train_dataloader, test_dataloader

def get_mnist_tensor():
    X, y = create_dataloaders(train_data,test_data)
    return X,y

train_loader, test_loader=get_mnist_tensor()

for X, y in train_loader:
        print(f"Shape of X [N, C, H, W]: {X.shape}")
        print(f"Shape of y: {y.shape} {y.dtype}")
        break

def cross_entropy(y_pred,y):
    v=-(y*torch.log(y_pred+0.00001))
    v.torch.mean(v)
    return v

class MyNN(nn.Module):
    def __init__(self,inp_dim=64,hid_dim=13,num_classes=10):
        super(MyNN,self).__init__()

        #convolution layers
        self.encoder1=nn.Conv2d(1,inp_dim,hid_dim)

        #transposed convolution layers
        self.decoder1=nn.ConvTranspose2d(hid_dim,6,num_classes)

        self.m=nn.Softmax(dim=1)
    
    def forward(self,x):
        x=F.relu(self.encoder1(x))
        #print(x.shape)  # [6,26,26]

        x=F.relu(self.decoder1(x))
        #print(x.shape)  # [10,26,26]
        x=self.m(x)

        x_dec=self.decoder1(x)

        return x,x_dec

  
    # This a multi component loss function - lc1 for class prediction loss and lc2 for auto-encoding loss
    def loss_fn(self,x,yground,y_pred,xencdec):
        # class prediction loss
        # yground needs to be one hot encoded - write your code
        lc1 = cross_entropy(y_pred,yground) # write your code for cross entropy between yground and y_pred, advised to use torch.mean()
        
        # auto encoding loss
        lc2 = torch.mean((x - xencdec)**2)
        
        lval = lc1 + lc2
    
        return lval

def get_mynn(inp_dim=64,hid_dim=13,num_classes=10):
    mynn = MyNN(inp_dim,hid_dim,num_classes)
    mynn.double()
    return mynn

device='cuda' if torch.cuda.is_available() else 'cpu'
device

mynn=get_mynn().to(device)

def get_loss_on_single_point(mynn,x0,y0):
    y_pred, xencdec = mynn(x0)
    lossval = mynn.loss_fn(x0,y0,y_pred,xencdec)
    # the lossval should have grad_fn attribute set
    return lossval

x0,y0=train_data[0]
print(x0.shape)

x0=x0.to(device)
#y0=y0.to(device)

loss_single_point=get_loss_on_single_point(mynn,x0,y0)

def train_combined_encdec_predictor(mynn,X,y, epochs=11):
  # X, y are provided as tensor
  # perform training on the entire data set (no batches etc.)
  # for each epoch, update weights
  
  optimizer = optim.SGD(mynn.parameters(), lr=0.01)
  
  for i in range(epochs):
    optimizer.zero_grad()
    ypred, Xencdec = mynn(X)
    lval = mynn.loss_fn(X,y,ypred,Xencdec)
    lval.backward()
    optimizer.step()
    
  return mynn

